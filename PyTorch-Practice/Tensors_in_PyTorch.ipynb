{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensors_in_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joycechidi/Deep-Learning-/blob/master/PyTorch-Practice/Tensors_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCXBtIz4-GFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#First import PyTorch\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIzW0NjpD0zK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation(x):\n",
        "    \"\"\"\n",
        "       Sigmoid activation function\n",
        "    \n",
        "       Arguments\n",
        "       ----------\n",
        "       x: torch.Tensor\n",
        "    \"\"\"\n",
        "    return 1/(1+torch.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jeghUmDD0u5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generate some random data\n",
        "torch.manual_seed(7) #Set the random seed so things are predictable\n",
        "\n",
        "#Features are 5 random normal variables\n",
        "features = torch.randn((1, 5))\n",
        "\n",
        "#True weights for our data, random normal variables again\n",
        "weights = torch.randn_like(features)\n",
        "\n",
        "# and a true bias term\n",
        "bias = torch.randn((1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzATq_g5H_7k",
        "colab_type": "text"
      },
      "source": [
        "I will use actual data in my notebooks with time.\n",
        "\n",
        "I'm going to do the following:\n",
        "\n",
        "*   Calculate the output of the network with input features features, weights weights, and bias bias.\n",
        "*   Use the function activation defined above as the activation function.\n",
        "\n",
        "Similar to Numpy, PyTorch has a torch.sum() function, as well as a .sum() method on tensors, for taking sums. I prefer to use matrix multiplication of the features and weights since it is more efficient and accelerated using modern libraries and high-performance computing on GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK9PL8z5D0q-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "8aca7a10-2624-463f-aa1f-d33f9c8bb586"
      },
      "source": [
        "output = activation(torch.matmul(features, weights))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-326cdc5cb1c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 5], m2: [1 x 5] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:752"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us24Q8uZLMdu",
        "colab_type": "text"
      },
      "source": [
        "Running the above code as will return a size mismatch error. The shape of the features and weights are very important in running these calculations. In the next cell, I will transpose the weight in order to change the shape of the tensor from 1 x 5 to 5 x 1 so the calculation can go through."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZXLT7YtD0oB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5309d212-8f2e-418e-9a6e-4ef2e80f8a53"
      },
      "source": [
        "output = activation(torch.matmul(features, weights.T) + bias)\n",
        "output"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1595]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWgmNqFvPv9z",
        "colab_type": "text"
      },
      "source": [
        "I can also use the view() to manually change the shape of the weights and still get the tensor values as seen in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kv39bVIQvZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d20d7476-3048-4d78-f7a8-d3cbbdf78efd"
      },
      "source": [
        "output = activation(torch.mm(features, weights.view(5, 1)) + bias)\n",
        "output"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1595]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZW1smEPPzzr",
        "colab_type": "text"
      },
      "source": [
        "### Stack them up!\n",
        "\n",
        "That's how you can calculate the output for a single neuron. The real power of this algorithm happens when you start stacking these individual units into layers and stacks of layers, into a network of neurons. The output of one layer of neurons becomes the input for the next layer. With multiple input units and output units, we now need to express the weights as a matrix.\n",
        "\n",
        "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
        "\n",
        "The first layer shown on the bottom here are the inputs, understandably called the **input layer**. The middle layer is called the **hidden layer**, and the final layer (on the right) is the **output layer**. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated \n",
        "\n",
        "$$\n",
        "\\vec{h} = [h_1 \\, h_2] = \n",
        "\\begin{bmatrix}\n",
        "x_1 \\, x_2 \\cdots \\, x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot \n",
        "\\begin{bmatrix}\n",
        "           w_{11} & w_{12} \\\\\n",
        "           w_{21} &w_{22} \\\\\n",
        "           \\vdots &\\vdots \\\\\n",
        "           w_{n1} &w_{n2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
        "\n",
        "$$\n",
        "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aFldEVuD0gP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Generate some data\n",
        "torch.manual_seed(7) # set the random seed to a number so things are preddictable\n",
        "\n",
        "#Features arre 3 random normal variables\n",
        "features = torch.randn((1, 3))\n",
        "\n",
        "#Define te size of each layer in our network\n",
        "n_input = features.shape[1]\n",
        "#number of inputs must match number of input features\n",
        "n_hidden = 2  #number of hidden units\n",
        "n_output = 1  #number of output units\n",
        "\n",
        "\n",
        "#Weights for inputs to hidden layer\n",
        "W1 = torch.randn(n_input, n_hidden)\n",
        "#Weights for hidden layer to output layer\n",
        "W2 = torch.randn(n_hidden, n_output)\n",
        "\n",
        "# and bias terms for hidden and output layers\n",
        "B1 = torch.randn((1, n_hidden))\n",
        "B2 = torch.randn((1, n_output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiSDO-WFv-WT",
        "colab_type": "text"
      },
      "source": [
        "Let's calculate the output for this multi-layer network using weightgs W1 & W2, and the biases, B1 & B2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoLyLz-XPvQA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0447333c-0f65-42c8-8aac-82fdaf978a48"
      },
      "source": [
        "# Solution\n",
        "\n",
        "input_layer = activation(torch.mm(features, W1) + B1)\n",
        "output = activation(torch.mm(input_layer, W2) + B2)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3171]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-M0p-J_4Oq9",
        "colab_type": "text"
      },
      "source": [
        "The number of hidden units a parameter of the network has is often called a **hyperparameter**. This differentiates it from the weights and biases parameters.\n",
        "When training a neural network, the more hidden layers your network and the more layers it has, the better it will be able to learn from data and make accurate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIKn3LpU6LJ2",
        "colab_type": "text"
      },
      "source": [
        "Numpy to Torch (Vice versa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9POp4Y-6UfJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0dbca59b-064b-461f-fce4-3185e4e2ffab"
      },
      "source": [
        "import numpy as np\n",
        "a = np.random.rand(4, 3)\n",
        "a\n",
        "\n",
        "#torch.from_numpy()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.84627374, 0.80484474, 0.94360632],\n",
              "       [0.54594401, 0.23088077, 0.73241634],\n",
              "       [0.9551725 , 0.97671948, 0.17419294],\n",
              "       [0.28749573, 0.50676332, 0.43288085]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nKFvZLr4A5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2352eca5-f998-4822-bc48-8ebc909aabf3"
      },
      "source": [
        "b = torch.from_numpy(a)\n",
        "b"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8463, 0.8048, 0.9436],\n",
              "        [0.5459, 0.2309, 0.7324],\n",
              "        [0.9552, 0.9767, 0.1742],\n",
              "        [0.2875, 0.5068, 0.4329]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KPO8ChE63-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e9c93e28-d992-4d7b-e03b-25e203f95558"
      },
      "source": [
        "b.numpy()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.84627374, 0.80484474, 0.94360632],\n",
              "       [0.54594401, 0.23088077, 0.73241634],\n",
              "       [0.9551725 , 0.97671948, 0.17419294],\n",
              "       [0.28749573, 0.50676332, 0.43288085]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phivxmq_7CXC",
        "colab_type": "text"
      },
      "source": [
        "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU6onker637S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "057c3f9f-bbfc-4d62-9cae-90964b6f79f4"
      },
      "source": [
        "# Multipy a PyTorch tensor by 2, in place\n",
        "b.mul_(2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.6925, 1.6097, 1.8872],\n",
              "        [1.0919, 0.4618, 1.4648],\n",
              "        [1.9103, 1.9534, 0.3484],\n",
              "        [0.5750, 1.0135, 0.8658]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZWZbzBG6344",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9450ec9d-67ee-42cc-db48-6cea89a02b4d"
      },
      "source": [
        "# Numpy array matches new values from Tensor\n",
        "a"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.69254747, 1.60968948, 1.88721264],\n",
              "       [1.09188802, 0.46176154, 1.46483267],\n",
              "       [1.91034499, 1.95343897, 0.34838588],\n",
              "       [0.57499145, 1.01352663, 0.8657617 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGX0BWSO633I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOtk92EG631o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaMD74X963xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eJDcPn-63vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pd-_YGN63tK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb2azw2I63qC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}