{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated_Deep_Learning_in_Pytorch_Pysyft.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joycechidi/Deep-Learning-/blob/master/Federated_Deep_Learning_in_Pytorch_Pysyft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSj2YAwFh07W",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of Federated Learning on The Fashion-MNIST Dataset Using Pytorch and PySyft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbfSFXOCiX05",
        "colab_type": "text"
      },
      "source": [
        "**Overview:** This is one of my keystone projects for the Secure and Private Artificial Intelligence Scholarship Challenge\n",
        "All codes were written by Joyce Chidinma Chidiadi\n",
        "\n",
        "# Why use Federated Learning at all?\n",
        "Federated learning is a technique used to train decentralized data. The data can be in the hands of the users or owners while being trained as part of a model. The trained model from each user or device will be aggregated. this technique improves privacy and protects the owners data since the data will not leave the device. Federated learning promotes and supports the concept of data privacy and the GDPR in EU which enforces data protection for the users. It is a positive development in the use of data for building artificial intelligent models.\n",
        "\n",
        "According to Google AI research, \"Federated Learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud. This goes beyond the use of local models that make predictions on mobile devices (like the Mobile Vision API and On-Device Smart Reply) by bringing model training to the device as well.\"\n",
        "\n",
        "For more information about Federated learning, read this **[Google AI Blog here](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)**.\n",
        "\n",
        "I used the FashionMNIST dataset for this project which has 10 different clothing labels.\n",
        "\n",
        "This project is based on the**[ OpenMined Federated Learning tutorial found here]( https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEgFQS-s4lwt",
        "colab_type": "code",
        "outputId": "d6dc7d10-8124-4464-a6d0-cf9a3506dc41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syft in /usr/local/lib/python3.6/dist-packages (0.1.23a1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Requirement already satisfied: flask-socketio>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from syft) (4.2.1)\n",
            "Requirement already satisfied: msgpack>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from syft) (0.6.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.3)\n",
            "Requirement already satisfied: lz4>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from syft) (2.1.10)\n",
            "Requirement already satisfied: torchvision==0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: websocket-client>=0.56.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.56.0)\n",
            "Requirement already satisfied: tf-encrypted!=0.5.7,>=0.5.4 in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.8)\n",
            "Requirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: torch==1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.6/dist-packages (from syft) (8.0.2)\n",
            "Requirement already satisfied: zstd>=1.4.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.1.0)\n",
            "Requirement already satisfied: python-socketio>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from flask-socketio>=3.3.2->syft) (4.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0->syft) (1.12.0)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted!=0.5.7,>=0.5.4->syft) (5.1.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.5)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: python-engineio>=3.9.0 in /usr/local/lib/python3.6/dist-packages (from python-socketio>=4.3.0->flask-socketio>=3.3.2->syft) (3.9.3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr9qXi7yvML5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "ce63dcd0-58d4-4a5b-a54f-5c4503269366"
      },
      "source": [
        "!wget https://github.com/udacity/deep-learning-v2-pytorch/raw/master/intro-to-pytorch/helper.py"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-18 04:05:08--  https://github.com/udacity/deep-learning-v2-pytorch/raw/master/intro-to-pytorch/helper.py\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-to-pytorch/helper.py [following]\n",
            "--2019-08-18 04:05:08--  https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-to-pytorch/helper.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2813 (2.7K) [text/plain]\n",
            "Saving to: ‘helper.py.1’\n",
            "\n",
            "\rhelper.py.1           0%[                    ]       0  --.-KB/s               \rhelper.py.1         100%[===================>]   2.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-08-18 04:05:09 (22.5 MB/s) - ‘helper.py.1’ saved [2813/2813]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rB13-UZHiMN",
        "colab_type": "text"
      },
      "source": [
        "## **Importing Libraries and Model Specifications**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao72wOzP-0sT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import helper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymfBDjsk_T_N",
        "colab_type": "code",
        "outputId": "2f9238d2-5075-4fcf-ec25-259fd8c05bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import syft as sy  # <-- NEW: import the Pysyft library\n",
        "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
        "danny = sy.VirtualWorker(hook, id=\"danny\")  # <-- NEW: define remote worker dandon\n",
        "gogo = sy.VirtualWorker(hook, id=\"gogo\")  # <-- NEW: and daisy\n",
        "chidi = sy.VirtualWorker(hook, id=\"chidi\")\n",
        "ekele = sy.VirtualWorker(hook, id=\"ekele\")\n",
        "secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0818 04:05:19.659603 139659813341056 hook.py:98] Torch was already hooked... skipping hooking process\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKVxcBsb_UHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 10\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLjBAE-1_UPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = not args.no_cuda and torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXuUpUbw_UaB",
        "colab_type": "code",
        "outputId": "abee9f02-c810-4361-9d08-9c620f79b59b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.manual_seed(args.seed)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f04a65ddab0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCFdQyTB_UX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfmme2Lj_UVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPltNLsaAOQf",
        "colab_type": "text"
      },
      "source": [
        "# **Data loading and sending to workers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTn6Or-r_UUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "federated_trainloader = sy.FederatedDataLoader(\n",
        "    datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.5,), (0.5,))\n",
        "                   ])).federate((danny, gogo, chidi, ekele, secure_worker)),\n",
        "    batch_size = args.batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEa1x_fQ-PzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik1sQbvb_UNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.5,), (0.5,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7LUF8ZHAV5l",
        "colab_type": "text"
      },
      "source": [
        "# **Convolutional Neural Network Specification**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YREfGXgL_UML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4dee922c-af70-425c-b7c3-687390863a63"
      },
      "source": [
        "#Define the network architecture\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        \n",
        "#Defining the layers, 128, 64, 10 units each\n",
        "\n",
        "#     n_input = 784\n",
        "#     n_hidden = 256\n",
        "#     n_hidden1 = 128\n",
        "#     n_hidden2 = 64\n",
        "#     n_output = 10    #softmax\n",
        "\n",
        "#       self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "#       self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        \n",
        "        #Output layer, 10 units - one for each digit\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Forward pass through the network, returns the output logits.\n",
        "    Flatten the input tensors\n",
        "    '''\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.max_pool2d(x, 2, 2)\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.max_pool2d(x, 2, 2)\n",
        "        \n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        \n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "        \n",
        "        return x\n",
        "model = Classifier()\n",
        "model\n",
        "    \n",
        "    "
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifKtL55uybk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd0ca0d2-a59c-454a-c55a-6de8a65a903b"
      },
      "source": [
        "print(model.fc4.weight)\n",
        "print(model.fc4.bias)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0356, -0.0061,  0.0546,  0.1188, -0.0318, -0.0368,  0.1160, -0.0400,\n",
            "          0.1100, -0.0557,  0.0443, -0.0070,  0.0502, -0.0071,  0.0142, -0.0257,\n",
            "          0.0985, -0.0625,  0.0328,  0.0686,  0.0511, -0.0554,  0.0687,  0.0658,\n",
            "         -0.0039, -0.1152, -0.0325,  0.0950, -0.1140, -0.0028,  0.0978, -0.0395,\n",
            "          0.1249, -0.0460,  0.0468,  0.0313,  0.0490,  0.0069,  0.1026,  0.1041,\n",
            "          0.0615,  0.0562, -0.1147, -0.1154,  0.0918, -0.0345,  0.0323,  0.0378,\n",
            "         -0.0436,  0.0109,  0.0399, -0.0996, -0.0770, -0.0501,  0.0301, -0.1050,\n",
            "         -0.0352, -0.0502,  0.0117,  0.0885,  0.0987, -0.0655,  0.0712,  0.0138],\n",
            "        [-0.0653,  0.0090, -0.0626, -0.0391,  0.0328, -0.0726, -0.0916, -0.0386,\n",
            "         -0.0836, -0.0555, -0.0764, -0.0467,  0.0533,  0.0157, -0.0890, -0.1126,\n",
            "          0.1206, -0.0478,  0.0104,  0.0393,  0.0642,  0.0255, -0.1102, -0.0516,\n",
            "         -0.0396, -0.0782, -0.1079, -0.0292, -0.0044, -0.0988,  0.0385,  0.0471,\n",
            "         -0.0450,  0.0623,  0.0637, -0.0758,  0.0993, -0.0656, -0.1036, -0.0402,\n",
            "          0.0577, -0.0132, -0.1127,  0.1074,  0.0811, -0.1208,  0.0799, -0.1238,\n",
            "          0.0528, -0.0091, -0.0202,  0.0837,  0.0933, -0.0605, -0.1193, -0.0469,\n",
            "          0.0344,  0.0532,  0.0402, -0.0745,  0.0258, -0.0528, -0.1182, -0.0784],\n",
            "        [ 0.1069,  0.0489, -0.0131,  0.0430,  0.0705,  0.0940,  0.1087, -0.1186,\n",
            "          0.0199, -0.0652, -0.0450, -0.0386,  0.0091,  0.0458,  0.0959,  0.1166,\n",
            "         -0.0855,  0.0909,  0.0302,  0.1095, -0.0318, -0.1016,  0.0600,  0.0693,\n",
            "          0.0836,  0.0484,  0.0112, -0.0933, -0.0275, -0.0819,  0.0327,  0.0021,\n",
            "          0.0853, -0.0602,  0.0094,  0.0347, -0.0421,  0.0043, -0.1153, -0.1072,\n",
            "         -0.0226, -0.0034,  0.0383, -0.0549, -0.0093,  0.0227, -0.0284,  0.0365,\n",
            "         -0.0188, -0.0739, -0.0520, -0.0577,  0.1100,  0.0122,  0.0813,  0.0786,\n",
            "         -0.0688, -0.0802,  0.0040, -0.0011, -0.0652, -0.0529,  0.1084, -0.0578],\n",
            "        [ 0.0540,  0.0852, -0.1104,  0.0916,  0.1219,  0.1217, -0.0133, -0.0172,\n",
            "         -0.0048,  0.1022, -0.0573,  0.0988,  0.0108, -0.1084,  0.1216, -0.0395,\n",
            "          0.0206, -0.0968, -0.0397, -0.0583, -0.0921,  0.0067, -0.0509,  0.1228,\n",
            "          0.1073, -0.0904, -0.0641,  0.0759,  0.0517,  0.0402,  0.1238,  0.0678,\n",
            "          0.0676, -0.0258, -0.1100,  0.0152, -0.0579,  0.1071, -0.0691,  0.1043,\n",
            "          0.0146,  0.0583,  0.0531,  0.1062,  0.0338,  0.0392,  0.0961,  0.0359,\n",
            "         -0.0151,  0.1144,  0.0445,  0.0161, -0.0885, -0.0073,  0.0961,  0.0985,\n",
            "          0.0734, -0.0014,  0.1146,  0.1067, -0.0763, -0.1034,  0.1098,  0.0118],\n",
            "        [ 0.0427,  0.0800, -0.0767, -0.0931,  0.0544,  0.0291,  0.0952,  0.0035,\n",
            "         -0.0053,  0.0413, -0.0364,  0.0636, -0.1087,  0.0887, -0.0314,  0.0159,\n",
            "         -0.1107,  0.0689, -0.0667, -0.1002, -0.0088,  0.0974,  0.0269, -0.0680,\n",
            "          0.0903,  0.1011,  0.0010, -0.0035,  0.0496, -0.0892,  0.0693,  0.0794,\n",
            "          0.0707, -0.0913, -0.0282,  0.0013,  0.1118, -0.0935,  0.0691, -0.0725,\n",
            "         -0.0321, -0.0288, -0.1130, -0.0573, -0.1137,  0.0843, -0.0958, -0.0450,\n",
            "         -0.0465, -0.0993, -0.0022,  0.1248, -0.0907, -0.1159,  0.0248, -0.0134,\n",
            "          0.1045,  0.0727, -0.0257,  0.0986, -0.0810, -0.1027,  0.0635,  0.0501],\n",
            "        [ 0.0915,  0.0543, -0.0545, -0.0976,  0.0229, -0.0488,  0.1014,  0.0571,\n",
            "          0.0904, -0.0369,  0.0755,  0.0047,  0.0886,  0.0716,  0.0727,  0.1175,\n",
            "          0.0568,  0.0150,  0.0926,  0.1112,  0.0199,  0.0086, -0.0725, -0.0150,\n",
            "         -0.0058,  0.0232,  0.0662,  0.0954,  0.0420, -0.0515, -0.1095,  0.0142,\n",
            "          0.0716, -0.1248,  0.0084, -0.0907,  0.0715,  0.0963,  0.0136,  0.0150,\n",
            "          0.0329,  0.0798, -0.0476, -0.0287, -0.0002,  0.0519, -0.0881, -0.0695,\n",
            "         -0.0517, -0.0655,  0.0468, -0.0859, -0.0818, -0.0325, -0.0430,  0.0126,\n",
            "          0.0926,  0.1072, -0.0844, -0.0559, -0.1033,  0.0634, -0.0996,  0.1206],\n",
            "        [ 0.1047, -0.0577,  0.0381, -0.0922, -0.0920, -0.0041, -0.0659, -0.0294,\n",
            "          0.0375, -0.0715, -0.0978, -0.0727, -0.0998, -0.1197, -0.0004, -0.1004,\n",
            "          0.0116,  0.0971,  0.0858, -0.1177, -0.0357,  0.0782, -0.0269, -0.0067,\n",
            "          0.0971, -0.0518,  0.1033,  0.0554,  0.0280, -0.0776, -0.0852,  0.1147,\n",
            "         -0.0249,  0.0191,  0.1169, -0.0580,  0.0862, -0.0925, -0.1007, -0.0858,\n",
            "          0.0507,  0.0369, -0.0402,  0.0632,  0.0737, -0.0742, -0.0206,  0.0715,\n",
            "         -0.0355, -0.0189, -0.0610,  0.1242,  0.0677,  0.0068,  0.0975, -0.0944,\n",
            "         -0.0985, -0.1235, -0.0243,  0.0682, -0.1236,  0.1128,  0.0443,  0.0547],\n",
            "        [ 0.0160, -0.0606, -0.0754, -0.0760,  0.0354,  0.0449,  0.0082, -0.0042,\n",
            "         -0.0103, -0.0862, -0.0100, -0.0536,  0.0891, -0.0422,  0.1019, -0.0322,\n",
            "          0.0020,  0.0566, -0.0842,  0.0991,  0.0438, -0.0957, -0.0232, -0.0115,\n",
            "         -0.0663, -0.0500, -0.1101, -0.0175,  0.0709,  0.0308,  0.0228, -0.0193,\n",
            "         -0.0252, -0.0990, -0.1112,  0.0673,  0.0203,  0.0207,  0.0551, -0.1208,\n",
            "         -0.0910,  0.0762,  0.1221,  0.0641, -0.0576, -0.0635,  0.0254,  0.0104,\n",
            "          0.0546, -0.0983,  0.0697,  0.0158,  0.0411, -0.0928, -0.0562, -0.0514,\n",
            "         -0.0701,  0.0305,  0.1056, -0.1049, -0.0106,  0.0184,  0.0605,  0.1080],\n",
            "        [ 0.0233,  0.0422,  0.1227,  0.0680, -0.0319,  0.0942, -0.0264, -0.0892,\n",
            "          0.0203, -0.0506, -0.1246, -0.0031,  0.0172,  0.0234,  0.0389, -0.1101,\n",
            "         -0.0902,  0.0443, -0.0706, -0.0161,  0.0291, -0.1057,  0.0071,  0.0641,\n",
            "         -0.1022, -0.0818, -0.1058, -0.1079,  0.0366,  0.0671, -0.0840, -0.0484,\n",
            "          0.0216, -0.0865,  0.0340,  0.0492, -0.0666,  0.0795, -0.1002, -0.0871,\n",
            "         -0.0356,  0.0361,  0.0319,  0.0677, -0.1164,  0.0669, -0.1218, -0.1037,\n",
            "          0.0641, -0.0739,  0.0578, -0.0317, -0.1053, -0.0302, -0.0071,  0.0319,\n",
            "          0.1169, -0.0282,  0.0588, -0.1154,  0.1249, -0.0099,  0.0810,  0.0731],\n",
            "        [-0.0058,  0.0258, -0.0986,  0.0101,  0.0188, -0.0982, -0.0436, -0.0698,\n",
            "          0.1118, -0.0524, -0.0685, -0.0677, -0.0036,  0.1098,  0.0830, -0.0337,\n",
            "          0.0481, -0.0949, -0.0362, -0.0742,  0.1038, -0.0448,  0.1091, -0.0204,\n",
            "         -0.0947,  0.0101,  0.0990,  0.0345,  0.0218,  0.0747, -0.1211,  0.0686,\n",
            "         -0.0507, -0.1101, -0.1019, -0.0209,  0.0580,  0.0022, -0.1047,  0.1087,\n",
            "         -0.0749,  0.0152,  0.1176,  0.1041, -0.0360, -0.0220,  0.0397, -0.0683,\n",
            "          0.0694,  0.0967,  0.0229, -0.0308,  0.0763,  0.0848,  0.1046, -0.1174,\n",
            "          0.0174,  0.0829, -0.0821, -0.0275, -0.0303, -0.0640, -0.0817,  0.1117]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0028,  0.0472,  0.0834, -0.0825, -0.0963,  0.1103,  0.1053, -0.0551,\n",
            "         0.0455,  0.0177], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hkir831Aw3V",
        "colab_type": "text"
      },
      "source": [
        "# **Define the Train and Test Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N28Kwpcv_UFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, federated_trainloader, optimizer, epoch):\n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(federated_trainloader): # <-- now it is a distributed dataset\n",
        "        model.send(data.location) # <-- NEW: send the model to the right location\n",
        "        \n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()        \n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        model.get() # <-- NEW: get the model back\n",
        "        \n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(federated_trainloader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(federated_trainloader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiAzsGIS_UDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmrDdW7EBMJI",
        "colab_type": "text"
      },
      "source": [
        "# **Launching The Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkfA6uw4BITg",
        "colab_type": "code",
        "outputId": "bf41a578-afb8-40e5-9637-f9e12e8edbf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Classifier().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_trainloader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"fashion_mnist_cnn.pt\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.298690\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.301392\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.286970\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.295721\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 2.283932\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 2.281917\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 2.268780\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 2.274123\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 2.253397\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 2.261686\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 2.239564\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 2.243096\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 2.210233\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 2.205749\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 2.210303\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 2.175116\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 2.151475\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 2.143023\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 2.141237\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 2.081135\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 2.111868\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 2.054115\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 2.001614\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 1.944003\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 1.885994\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 1.828490\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 1.808258\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 1.767714\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 1.677225\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 1.721115\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 1.769762\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 1.574266\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 1.574550\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 1.598478\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 1.501194\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 1.491209\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 1.430532\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 1.389150\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 1.386047\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 1.295743\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 1.226545\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 1.270326\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 1.231838\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 1.327074\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 1.152438\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 1.073636\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 1.137534\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 1.091734\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 1.047912\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 1.047099\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 1.093787\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 1.076642\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 1.018365\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 1.066095\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 1.193845\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.870756\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.894706\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 1.087539\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.890422\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.846211\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.871287\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.903677\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.977487\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.827008\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 0.757800\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 0.868661\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.759425\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 0.898870\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.846848\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.981536\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.919794\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.822395\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.687507\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.865449\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.782685\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.858826\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.697778\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.953365\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.816136\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.681536\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.817807\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.734435\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.754783\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.825124\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.772454\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.776722\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.658085\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.717497\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.657110\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.531701\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.845193\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.768289\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.760496\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.703957\n",
            "\n",
            "Test set: Average loss: 0.7446, Accuracy: 7113/10000 (71%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.632976\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.815378\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.775124\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.678096\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.704104\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.570758\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.707835\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.734665\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.825331\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.831253\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.655896\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.892262\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.741425\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.733023\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.653276\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.774113\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.625914\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.830500\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.701427\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.629180\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.700781\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.599167\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.436350\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.570444\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.725886\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.713205\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.629292\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.694899\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.615071\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.549798\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.640068\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.573783\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 0.590813\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.774386\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.674588\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.663818\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.913516\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.792967\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.566511\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.633532\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.726456\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.764122\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.528082\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.349765\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.621421\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.585279\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.829205\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 1.063846\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.679277\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.586683\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.716256\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.598880\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.585029\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.479948\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.665430\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.645740\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.613676\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.432528\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.581314\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.650220\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.749336\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.580734\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.737831\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.694579\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 0.498565\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.602791\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.667016\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.684135\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.674581\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.696044\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.575581\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.643592\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.512420\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.785172\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.706739\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.542349\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.705210\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.534350\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.772906\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.650655\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.646064\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.741891\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.637484\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.449809\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.675380\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.523296\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.343436\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.695074\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.540455\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.604073\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.596917\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.714083\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.625223\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.670892\n",
            "\n",
            "Test set: Average loss: 0.6201, Accuracy: 7695/10000 (77%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.678909\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.388853\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.530800\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.445062\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.535536\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.665536\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.478063\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.466427\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.503701\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.647458\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.617773\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.595818\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.454126\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.513193\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.626777\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.542654\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.330882\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.459045\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.586697\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.508744\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.595424\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.509744\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.742429\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.732617\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.578876\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.581921\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.513703\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.451880\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.461245\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.492543\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.472672\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.426137\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 0.459200\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.534964\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.247703\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.652446\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.414066\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.717514\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.365950\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.701300\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.534294\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.681691\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.570771\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.581595\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.475126\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.560369\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.596282\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.544693\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.580529\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.458744\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.613944\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.559864\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.545033\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.669724\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.558793\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.472338\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.560909\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.538118\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.693834\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.828284\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.397455\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.437261\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.558783\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.489827\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.713420\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.501109\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.511925\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.625387\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.672382\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.496638\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.582132\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.496751\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.531726\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.683422\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.399480\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.703007\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.611699\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.675981\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.664653\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.646552\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.627747\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.423174\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.623157\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.601471\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.543072\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.442813\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.586599\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.510190\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.606735\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.458805\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.562746\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.500056\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.592938\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.559545\n",
            "\n",
            "Test set: Average loss: 0.5360, Accuracy: 8052/10000 (81%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.552236\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.687714\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.613706\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.659225\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.388961\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.341022\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.425256\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.669091\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.609320\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.455116\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.521954\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.372432\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.513501\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.478964\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.564964\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.607195\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.518181\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.414945\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.632100\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.420748\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.568233\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.362643\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.478830\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.361821\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.452818\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.555799\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.556140\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.498761\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.722045\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.362065\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.520036\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.518614\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 0.596503\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.531430\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.402683\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.476078\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.457420\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.481235\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.477471\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.464689\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.667836\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.621322\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.470176\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.840363\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.582266\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.538028\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.431151\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.532405\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.685307\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.445962\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.384584\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.444170\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.547621\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.576511\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.550571\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.380288\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.487023\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.577034\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.440037\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.410885\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.540886\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.699821\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.599874\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.494228\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.416147\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.451842\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.625646\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.394797\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.395220\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.549343\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.494556\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.566114\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.377946\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.542860\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.451245\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.527966\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.632322\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.369451\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.491205\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.546511\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.460439\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.515252\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.366715\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.476806\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.447371\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.554868\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.636942\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.349398\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.257968\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.458298\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.461951\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.459594\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.694511\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.288579\n",
            "\n",
            "Test set: Average loss: 0.5217, Accuracy: 8101/10000 (81%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.592285\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.492366\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.390519\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.504769\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.420939\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.369839\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.530041\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.486879\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.507032\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.622527\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.338451\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.574077\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.715083\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.534298\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.392429\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.445962\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.772442\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.377292\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.409160\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.576460\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.596442\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.467341\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.460149\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.480229\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.625074\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.397534\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.573466\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.600015\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.630227\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.363137\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.428163\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.462124\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.379543\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.507385\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.481295\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.449544\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.513126\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.339029\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.343632\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.463610\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.372770\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.417180\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.431905\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.470013\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.390627\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.670816\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.582226\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.541188\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.469278\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.545402\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.432478\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.355287\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.744645\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.240515\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.434588\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.368300\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.497148\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.258156\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.533128\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.364405\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.527136\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.425404\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.378434\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.518439\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.519859\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.416506\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.463375\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.453169\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.470414\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.426246\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.630574\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.472543\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.565611\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.400645\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.329038\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.420901\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.359567\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.642901\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.438714\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.304246\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.340240\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.543182\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.369925\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.422757\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.314696\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.619943\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.424971\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.441986\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.440946\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.540252\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.477924\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.825735\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.497871\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.463936\n",
            "\n",
            "Test set: Average loss: 0.4813, Accuracy: 8286/10000 (83%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.359673\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: 0.444109\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 0.418048\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.449317\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.294530\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.487089\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.287097\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.641617\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.401287\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.396969\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.421958\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.324351\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.454049\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.486453\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.352708\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.442043\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.694238\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.455021\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.465937\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.377791\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.337276\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.614572\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.496405\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.491352\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.415143\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.416775\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.421645\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.537567\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.497961\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.341938\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.382215\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.532419\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.432038\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.367552\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 0.460430\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.518347\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.507733\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.431290\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.383654\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.561797\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.506810\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.415399\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.342520\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.437070\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.427118\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.415721\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.330675\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.324641\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.379907\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.473380\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.270669\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.442867\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.589266\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.274342\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.401969\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.516821\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.284322\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.284935\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.478979\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.320510\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.388656\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.674225\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.435791\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.551380\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 0.740802\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.613566\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.272930\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.358273\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.367079\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.339644\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.506868\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.462002\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.429593\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.410013\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.469474\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.259155\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.282267\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.308615\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.267582\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.275148\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.590164\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.452087\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.256645\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.549274\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.364241\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.367496\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.470868\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.750489\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.472151\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.590237\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.550242\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.406478\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.389066\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.466697\n",
            "\n",
            "Test set: Average loss: 0.4613, Accuracy: 8371/10000 (84%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.327186\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.317850\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.444672\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.443092\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.344755\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.515670\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.303484\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.448467\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.442403\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.392772\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.572613\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.345731\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.365269\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.396053\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.467248\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.413312\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.607246\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.489249\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.323120\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.402079\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.395939\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.497605\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.349215\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.227193\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.393568\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.514625\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.348785\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.623255\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.405889\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.444603\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.546326\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.367976\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.325113\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.447922\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.525950\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.551702\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.699820\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.258844\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.303979\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.443717\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.291085\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.503268\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.432735\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.630643\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.350252\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.619922\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.394643\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.344296\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.316793\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.375124\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.275024\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.384178\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.426081\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.367230\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.503425\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.313708\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.569200\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.286622\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.271783\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.240327\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.466426\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.511696\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.323238\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.159090\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 0.341937\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.412802\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.291079\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.522623\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.614201\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.403025\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.559855\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.429406\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.428734\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.293177\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.589647\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.291665\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.454395\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.308836\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.329353\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.491510\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.479456\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.527672\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.367091\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.694006\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.357907\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.444733\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.423811\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.454051\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.325881\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.443083\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.350654\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.372118\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.362597\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.294609\n",
            "\n",
            "Test set: Average loss: 0.4616, Accuracy: 8341/10000 (83%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.261289\n",
            "Train Epoch: 8 [640/60032 (1%)]\tLoss: 0.340113\n",
            "Train Epoch: 8 [1280/60032 (2%)]\tLoss: 0.383036\n",
            "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.306980\n",
            "Train Epoch: 8 [2560/60032 (4%)]\tLoss: 0.466340\n",
            "Train Epoch: 8 [3200/60032 (5%)]\tLoss: 0.368840\n",
            "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.253210\n",
            "Train Epoch: 8 [4480/60032 (7%)]\tLoss: 0.325881\n",
            "Train Epoch: 8 [5120/60032 (9%)]\tLoss: 0.372428\n",
            "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.423722\n",
            "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.264634\n",
            "Train Epoch: 8 [7040/60032 (12%)]\tLoss: 0.297593\n",
            "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.264582\n",
            "Train Epoch: 8 [8320/60032 (14%)]\tLoss: 0.220125\n",
            "Train Epoch: 8 [8960/60032 (15%)]\tLoss: 0.261286\n",
            "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.403098\n",
            "Train Epoch: 8 [10240/60032 (17%)]\tLoss: 0.367139\n",
            "Train Epoch: 8 [10880/60032 (18%)]\tLoss: 0.497421\n",
            "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.401171\n",
            "Train Epoch: 8 [12160/60032 (20%)]\tLoss: 0.362904\n",
            "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.574099\n",
            "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.469383\n",
            "Train Epoch: 8 [14080/60032 (23%)]\tLoss: 0.389368\n",
            "Train Epoch: 8 [14720/60032 (25%)]\tLoss: 0.319199\n",
            "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.286278\n",
            "Train Epoch: 8 [16000/60032 (27%)]\tLoss: 0.383775\n",
            "Train Epoch: 8 [16640/60032 (28%)]\tLoss: 0.248832\n",
            "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.682347\n",
            "Train Epoch: 8 [17920/60032 (30%)]\tLoss: 0.521192\n",
            "Train Epoch: 8 [18560/60032 (31%)]\tLoss: 0.406406\n",
            "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.310978\n",
            "Train Epoch: 8 [19840/60032 (33%)]\tLoss: 0.361706\n",
            "Train Epoch: 8 [20480/60032 (34%)]\tLoss: 0.380020\n",
            "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.646915\n",
            "Train Epoch: 8 [21760/60032 (36%)]\tLoss: 0.416591\n",
            "Train Epoch: 8 [22400/60032 (37%)]\tLoss: 0.338420\n",
            "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.387826\n",
            "Train Epoch: 8 [23680/60032 (39%)]\tLoss: 0.372200\n",
            "Train Epoch: 8 [24320/60032 (41%)]\tLoss: 0.433651\n",
            "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.523432\n",
            "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.410755\n",
            "Train Epoch: 8 [26240/60032 (44%)]\tLoss: 0.327794\n",
            "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.358063\n",
            "Train Epoch: 8 [27520/60032 (46%)]\tLoss: 0.380433\n",
            "Train Epoch: 8 [28160/60032 (47%)]\tLoss: 0.280175\n",
            "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.348900\n",
            "Train Epoch: 8 [29440/60032 (49%)]\tLoss: 0.217380\n",
            "Train Epoch: 8 [30080/60032 (50%)]\tLoss: 0.474751\n",
            "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.402100\n",
            "Train Epoch: 8 [31360/60032 (52%)]\tLoss: 0.409654\n",
            "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.469301\n",
            "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.444159\n",
            "Train Epoch: 8 [33280/60032 (55%)]\tLoss: 0.485102\n",
            "Train Epoch: 8 [33920/60032 (57%)]\tLoss: 0.576248\n",
            "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.350044\n",
            "Train Epoch: 8 [35200/60032 (59%)]\tLoss: 0.456057\n",
            "Train Epoch: 8 [35840/60032 (60%)]\tLoss: 0.281853\n",
            "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.230133\n",
            "Train Epoch: 8 [37120/60032 (62%)]\tLoss: 0.413894\n",
            "Train Epoch: 8 [37760/60032 (63%)]\tLoss: 0.313650\n",
            "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.448729\n",
            "Train Epoch: 8 [39040/60032 (65%)]\tLoss: 0.344736\n",
            "Train Epoch: 8 [39680/60032 (66%)]\tLoss: 0.526075\n",
            "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.405426\n",
            "Train Epoch: 8 [40960/60032 (68%)]\tLoss: 0.396645\n",
            "Train Epoch: 8 [41600/60032 (69%)]\tLoss: 0.358842\n",
            "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.294877\n",
            "Train Epoch: 8 [42880/60032 (71%)]\tLoss: 0.284050\n",
            "Train Epoch: 8 [43520/60032 (72%)]\tLoss: 0.393403\n",
            "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.428542\n",
            "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.364537\n",
            "Train Epoch: 8 [45440/60032 (76%)]\tLoss: 0.342960\n",
            "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.329827\n",
            "Train Epoch: 8 [46720/60032 (78%)]\tLoss: 0.254042\n",
            "Train Epoch: 8 [47360/60032 (79%)]\tLoss: 0.462538\n",
            "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.238365\n",
            "Train Epoch: 8 [48640/60032 (81%)]\tLoss: 0.243394\n",
            "Train Epoch: 8 [49280/60032 (82%)]\tLoss: 0.442586\n",
            "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.387719\n",
            "Train Epoch: 8 [50560/60032 (84%)]\tLoss: 0.550850\n",
            "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.387534\n",
            "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.584238\n",
            "Train Epoch: 8 [52480/60032 (87%)]\tLoss: 0.333168\n",
            "Train Epoch: 8 [53120/60032 (88%)]\tLoss: 0.574966\n",
            "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.370808\n",
            "Train Epoch: 8 [54400/60032 (91%)]\tLoss: 0.262475\n",
            "Train Epoch: 8 [55040/60032 (92%)]\tLoss: 0.555748\n",
            "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.378060\n",
            "Train Epoch: 8 [56320/60032 (94%)]\tLoss: 0.381691\n",
            "Train Epoch: 8 [56960/60032 (95%)]\tLoss: 0.397097\n",
            "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.218950\n",
            "Train Epoch: 8 [58240/60032 (97%)]\tLoss: 0.291218\n",
            "Train Epoch: 8 [58880/60032 (98%)]\tLoss: 0.365651\n",
            "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.308436\n",
            "\n",
            "Test set: Average loss: 0.4441, Accuracy: 8398/10000 (84%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.221054\n",
            "Train Epoch: 9 [640/60032 (1%)]\tLoss: 0.396942\n",
            "Train Epoch: 9 [1280/60032 (2%)]\tLoss: 0.408124\n",
            "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.440989\n",
            "Train Epoch: 9 [2560/60032 (4%)]\tLoss: 0.403873\n",
            "Train Epoch: 9 [3200/60032 (5%)]\tLoss: 0.322184\n",
            "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.305790\n",
            "Train Epoch: 9 [4480/60032 (7%)]\tLoss: 0.202900\n",
            "Train Epoch: 9 [5120/60032 (9%)]\tLoss: 0.334371\n",
            "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.377465\n",
            "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.268565\n",
            "Train Epoch: 9 [7040/60032 (12%)]\tLoss: 0.196022\n",
            "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.430283\n",
            "Train Epoch: 9 [8320/60032 (14%)]\tLoss: 0.469252\n",
            "Train Epoch: 9 [8960/60032 (15%)]\tLoss: 0.345120\n",
            "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.286141\n",
            "Train Epoch: 9 [10240/60032 (17%)]\tLoss: 0.427539\n",
            "Train Epoch: 9 [10880/60032 (18%)]\tLoss: 0.330246\n",
            "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.282756\n",
            "Train Epoch: 9 [12160/60032 (20%)]\tLoss: 0.704489\n",
            "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.411211\n",
            "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.298907\n",
            "Train Epoch: 9 [14080/60032 (23%)]\tLoss: 0.391927\n",
            "Train Epoch: 9 [14720/60032 (25%)]\tLoss: 0.347285\n",
            "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.574678\n",
            "Train Epoch: 9 [16000/60032 (27%)]\tLoss: 0.396373\n",
            "Train Epoch: 9 [16640/60032 (28%)]\tLoss: 0.310573\n",
            "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.310338\n",
            "Train Epoch: 9 [17920/60032 (30%)]\tLoss: 0.402106\n",
            "Train Epoch: 9 [18560/60032 (31%)]\tLoss: 0.471527\n",
            "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.314015\n",
            "Train Epoch: 9 [19840/60032 (33%)]\tLoss: 0.262218\n",
            "Train Epoch: 9 [20480/60032 (34%)]\tLoss: 0.391756\n",
            "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.282654\n",
            "Train Epoch: 9 [21760/60032 (36%)]\tLoss: 0.442547\n",
            "Train Epoch: 9 [22400/60032 (37%)]\tLoss: 0.356608\n",
            "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.401404\n",
            "Train Epoch: 9 [23680/60032 (39%)]\tLoss: 0.407986\n",
            "Train Epoch: 9 [24320/60032 (41%)]\tLoss: 0.284183\n",
            "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.445559\n",
            "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.663977\n",
            "Train Epoch: 9 [26240/60032 (44%)]\tLoss: 0.309892\n",
            "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.303505\n",
            "Train Epoch: 9 [27520/60032 (46%)]\tLoss: 0.613901\n",
            "Train Epoch: 9 [28160/60032 (47%)]\tLoss: 0.440521\n",
            "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.470926\n",
            "Train Epoch: 9 [29440/60032 (49%)]\tLoss: 0.406117\n",
            "Train Epoch: 9 [30080/60032 (50%)]\tLoss: 0.421124\n",
            "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.440068\n",
            "Train Epoch: 9 [31360/60032 (52%)]\tLoss: 0.349200\n",
            "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.355620\n",
            "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.385472\n",
            "Train Epoch: 9 [33280/60032 (55%)]\tLoss: 0.473013\n",
            "Train Epoch: 9 [33920/60032 (57%)]\tLoss: 0.414139\n",
            "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.334506\n",
            "Train Epoch: 9 [35200/60032 (59%)]\tLoss: 0.308024\n",
            "Train Epoch: 9 [35840/60032 (60%)]\tLoss: 0.189531\n",
            "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.375207\n",
            "Train Epoch: 9 [37120/60032 (62%)]\tLoss: 0.414458\n",
            "Train Epoch: 9 [37760/60032 (63%)]\tLoss: 0.479351\n",
            "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.307350\n",
            "Train Epoch: 9 [39040/60032 (65%)]\tLoss: 0.233631\n",
            "Train Epoch: 9 [39680/60032 (66%)]\tLoss: 0.378125\n",
            "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.514414\n",
            "Train Epoch: 9 [40960/60032 (68%)]\tLoss: 0.408217\n",
            "Train Epoch: 9 [41600/60032 (69%)]\tLoss: 0.371539\n",
            "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.296418\n",
            "Train Epoch: 9 [42880/60032 (71%)]\tLoss: 0.451376\n",
            "Train Epoch: 9 [43520/60032 (72%)]\tLoss: 0.388056\n",
            "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.479761\n",
            "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.346021\n",
            "Train Epoch: 9 [45440/60032 (76%)]\tLoss: 0.309721\n",
            "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.387225\n",
            "Train Epoch: 9 [46720/60032 (78%)]\tLoss: 0.234852\n",
            "Train Epoch: 9 [47360/60032 (79%)]\tLoss: 0.261424\n",
            "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.458262\n",
            "Train Epoch: 9 [48640/60032 (81%)]\tLoss: 0.383389\n",
            "Train Epoch: 9 [49280/60032 (82%)]\tLoss: 0.309142\n",
            "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.361849\n",
            "Train Epoch: 9 [50560/60032 (84%)]\tLoss: 0.337898\n",
            "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.469118\n",
            "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.332255\n",
            "Train Epoch: 9 [52480/60032 (87%)]\tLoss: 0.328883\n",
            "Train Epoch: 9 [53120/60032 (88%)]\tLoss: 0.348370\n",
            "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.559875\n",
            "Train Epoch: 9 [54400/60032 (91%)]\tLoss: 0.354395\n",
            "Train Epoch: 9 [55040/60032 (92%)]\tLoss: 0.397096\n",
            "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.575404\n",
            "Train Epoch: 9 [56320/60032 (94%)]\tLoss: 0.343387\n",
            "Train Epoch: 9 [56960/60032 (95%)]\tLoss: 0.335107\n",
            "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.284671\n",
            "Train Epoch: 9 [58240/60032 (97%)]\tLoss: 0.500814\n",
            "Train Epoch: 9 [58880/60032 (98%)]\tLoss: 0.214891\n",
            "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.437799\n",
            "\n",
            "Test set: Average loss: 0.4264, Accuracy: 8480/10000 (85%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.367994\n",
            "Train Epoch: 10 [640/60032 (1%)]\tLoss: 0.395007\n",
            "Train Epoch: 10 [1280/60032 (2%)]\tLoss: 0.432069\n",
            "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.382238\n",
            "Train Epoch: 10 [2560/60032 (4%)]\tLoss: 0.375064\n",
            "Train Epoch: 10 [3200/60032 (5%)]\tLoss: 0.387459\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.345948\n",
            "Train Epoch: 10 [4480/60032 (7%)]\tLoss: 0.431513\n",
            "Train Epoch: 10 [5120/60032 (9%)]\tLoss: 0.402188\n",
            "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.340512\n",
            "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.400039\n",
            "Train Epoch: 10 [7040/60032 (12%)]\tLoss: 0.373298\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.421250\n",
            "Train Epoch: 10 [8320/60032 (14%)]\tLoss: 0.384128\n",
            "Train Epoch: 10 [8960/60032 (15%)]\tLoss: 0.202742\n",
            "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.433356\n",
            "Train Epoch: 10 [10240/60032 (17%)]\tLoss: 0.392464\n",
            "Train Epoch: 10 [10880/60032 (18%)]\tLoss: 0.378139\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.382465\n",
            "Train Epoch: 10 [12160/60032 (20%)]\tLoss: 0.417400\n",
            "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.354475\n",
            "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.524629\n",
            "Train Epoch: 10 [14080/60032 (23%)]\tLoss: 0.362518\n",
            "Train Epoch: 10 [14720/60032 (25%)]\tLoss: 0.429421\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.262565\n",
            "Train Epoch: 10 [16000/60032 (27%)]\tLoss: 0.527472\n",
            "Train Epoch: 10 [16640/60032 (28%)]\tLoss: 0.317667\n",
            "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.473551\n",
            "Train Epoch: 10 [17920/60032 (30%)]\tLoss: 0.318892\n",
            "Train Epoch: 10 [18560/60032 (31%)]\tLoss: 0.385426\n",
            "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.524462\n",
            "Train Epoch: 10 [19840/60032 (33%)]\tLoss: 0.434164\n",
            "Train Epoch: 10 [20480/60032 (34%)]\tLoss: 0.354042\n",
            "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.341842\n",
            "Train Epoch: 10 [21760/60032 (36%)]\tLoss: 0.432287\n",
            "Train Epoch: 10 [22400/60032 (37%)]\tLoss: 0.270920\n",
            "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.565894\n",
            "Train Epoch: 10 [23680/60032 (39%)]\tLoss: 0.336940\n",
            "Train Epoch: 10 [24320/60032 (41%)]\tLoss: 0.344985\n",
            "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.227426\n",
            "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.316788\n",
            "Train Epoch: 10 [26240/60032 (44%)]\tLoss: 0.361917\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.269893\n",
            "Train Epoch: 10 [27520/60032 (46%)]\tLoss: 0.306653\n",
            "Train Epoch: 10 [28160/60032 (47%)]\tLoss: 0.403595\n",
            "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.203754\n",
            "Train Epoch: 10 [29440/60032 (49%)]\tLoss: 0.606748\n",
            "Train Epoch: 10 [30080/60032 (50%)]\tLoss: 0.253682\n",
            "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.380272\n",
            "Train Epoch: 10 [31360/60032 (52%)]\tLoss: 0.382191\n",
            "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.228162\n",
            "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.293291\n",
            "Train Epoch: 10 [33280/60032 (55%)]\tLoss: 0.379742\n",
            "Train Epoch: 10 [33920/60032 (57%)]\tLoss: 0.290905\n",
            "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.326438\n",
            "Train Epoch: 10 [35200/60032 (59%)]\tLoss: 0.424600\n",
            "Train Epoch: 10 [35840/60032 (60%)]\tLoss: 0.494983\n",
            "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.309723\n",
            "Train Epoch: 10 [37120/60032 (62%)]\tLoss: 0.214054\n",
            "Train Epoch: 10 [37760/60032 (63%)]\tLoss: 0.384103\n",
            "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.390433\n",
            "Train Epoch: 10 [39040/60032 (65%)]\tLoss: 0.409071\n",
            "Train Epoch: 10 [39680/60032 (66%)]\tLoss: 0.534784\n",
            "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.288794\n",
            "Train Epoch: 10 [40960/60032 (68%)]\tLoss: 0.306946\n",
            "Train Epoch: 10 [41600/60032 (69%)]\tLoss: 0.539758\n",
            "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.642868\n",
            "Train Epoch: 10 [42880/60032 (71%)]\tLoss: 0.490100\n",
            "Train Epoch: 10 [43520/60032 (72%)]\tLoss: 0.597411\n",
            "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.300974\n",
            "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.350438\n",
            "Train Epoch: 10 [45440/60032 (76%)]\tLoss: 0.442194\n",
            "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.473683\n",
            "Train Epoch: 10 [46720/60032 (78%)]\tLoss: 0.310241\n",
            "Train Epoch: 10 [47360/60032 (79%)]\tLoss: 0.298094\n",
            "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.318451\n",
            "Train Epoch: 10 [48640/60032 (81%)]\tLoss: 0.274651\n",
            "Train Epoch: 10 [49280/60032 (82%)]\tLoss: 0.489290\n",
            "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.415825\n",
            "Train Epoch: 10 [50560/60032 (84%)]\tLoss: 0.337963\n",
            "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.230798\n",
            "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.384523\n",
            "Train Epoch: 10 [52480/60032 (87%)]\tLoss: 0.346546\n",
            "Train Epoch: 10 [53120/60032 (88%)]\tLoss: 0.228820\n",
            "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.413797\n",
            "Train Epoch: 10 [54400/60032 (91%)]\tLoss: 0.285027\n",
            "Train Epoch: 10 [55040/60032 (92%)]\tLoss: 0.443251\n",
            "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.430805\n",
            "Train Epoch: 10 [56320/60032 (94%)]\tLoss: 0.297042\n",
            "Train Epoch: 10 [56960/60032 (95%)]\tLoss: 0.423415\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.508665\n",
            "Train Epoch: 10 [58240/60032 (97%)]\tLoss: 0.423773\n",
            "Train Epoch: 10 [58880/60032 (98%)]\tLoss: 0.249388\n",
            "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.255714\n",
            "\n",
            "Test set: Average loss: 0.4041, Accuracy: 8571/10000 (86%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLOA5avmLVnP",
        "colab_type": "text"
      },
      "source": [
        "## **Testing The Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUu7_OlgBIat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "ddf13cee-42fc-4aa7-9e38-8ae2c6007081"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import helper \n",
        "\n",
        "#Test out your network!\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "img = images[3]\n",
        "\n",
        "#Calculate the class probabilities (softmax) for img\n",
        "\n",
        "ps = torch.exp(model(img))\n",
        "\n",
        "#plot the image and probabilities\n",
        "helper.view_classify(img, ps, version='Fashion')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAGdCAYAAAAllSMpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 423,
              "height": 206
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnS69B3n1WpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nLcqcTQBIfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssUiaqF5BIjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8tKKFUO_T8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
